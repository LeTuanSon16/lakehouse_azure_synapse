{
	"name": "Extract_data",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cfa5b7a1-4ab9-4c19-a4a0-e4e852f59cd7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a27e5480-6d37-4b13-ae06-aed35c4c2aea/resourceGroups/tmrlakehouse/providers/Microsoft.Synapse/workspaces/tmrlakehouse/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://tmrlakehouse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import sys\n",
					"import os\n",
					"import json\n",
					"import requests\n",
					"import pytz\n",
					"from datetime import datetime\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, from_unixtime, lit, current_timestamp\n",
					"from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
					"\n",
					"# Initialize Spark Session\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"ADLS Data Processing\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# ADLS Gen2 Configuration\n",
					"storage_account_name = \"tmrbronzelayer\"\n",
					"container_name = \"securities-price\"\n",
					"output_directory = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/data/\"\n",
					"\n",
					"def get_currentprice(urlPrice, symbol, resolution):\n",
					"    response = requests.get(urlPrice)\n",
					"    priceParsed = json.loads(response.text)\n",
					" \n",
					"    if \"t\" not in priceParsed:\n",
					"        print(f\"No data found for {symbol} at resolution {resolution}\")\n",
					"        return None\n",
					"\n",
					"    # Chuyển đổi timestamps\n",
					"    timestamps = priceParsed[\"t\"]\n",
					"\n",
					"    data = [(datetime.fromtimestamp(t, pytz.utc).astimezone(pytz.timezone(\"Asia/Ho_Chi_Minh\")),\n",
					"             datetime.fromtimestamp(t, pytz.utc).astimezone(pytz.timezone(\"Asia/Ho_Chi_Minh\")).strftime('%Y-%m-%d'),\n",
					"             int(resolution),\n",
					"             symbol,\n",
					"             float(priceParsed[\"o\"][i]),\n",
					"             float(priceParsed[\"h\"][i]),\n",
					"             float(priceParsed[\"l\"][i]),\n",
					"             float(priceParsed[\"c\"][i]),\n",
					"             float(priceParsed[\"v\"][i]),\n",
					"             datetime.now(),\n",
					"             datetime.now())\n",
					"            for i, t in enumerate(timestamps)]\n",
					"\n",
					"    # Định nghĩa schema\n",
					"    schema = StructType([\n",
					"        StructField(\"data_time\", TimestampType(), True),\n",
					"        StructField(\"data_date\", StringType(), True),\n",
					"        StructField(\"resolution\", IntegerType(), True),\n",
					"        StructField(\"ticker\", StringType(), True),\n",
					"        StructField(\"open_price\", DoubleType(), True),\n",
					"        StructField(\"highest_price\", DoubleType(), True),\n",
					"        StructField(\"lowest_price\", DoubleType(), True),\n",
					"        StructField(\"close_price\", DoubleType(), True),\n",
					"        StructField(\"volume\", DoubleType(), True),\n",
					"        StructField(\"created_at\", TimestampType(), True),\n",
					"        StructField(\"updated_at\", TimestampType(), True),\n",
					"    ])\n",
					"\n",
					"    # Tạo DataFrame Spark\n",
					"    df = spark.createDataFrame(data, schema=schema)\n",
					"    return df\n",
					"\n",
					"# Hàm tổng hợp dữ liệu từ danh sách mã chứng khoán\n",
					"def aggregate_data(startdate, enddate, resolution, urlPrice, tickers):\n",
					"    all_dataframes = []\n",
					"\n",
					"    for ticker in tickers:\n",
					"        start_ts = int(startdate.timestamp())\n",
					"        end_ts = int(enddate.timestamp())\n",
					"        url = f\"{urlPrice}symbol={ticker}&resolution={resolution}&from={start_ts}&to={end_ts}\"\n",
					"\n",
					"        df = get_currentprice(url, ticker, resolution)\n",
					"        if df is not None:\n",
					"            all_dataframes.append(df)\n",
					"\n",
					"    if all_dataframes:\n",
					"        final_df = all_dataframes[0]\n",
					"        for df in all_dataframes[1:]:\n",
					"            final_df = final_df.union(df)\n",
					"        return final_df\n",
					"    else:\n",
					"        return None\n",
					"\n",
					"def process(startdate, enddate, resolution, urlPrice):\n",
					"    print(f\"Processing for resolution {resolution}...\")\n",
					"    \n",
					"    tickers = [\"VN30F1M\"]\n",
					"    final_df = aggregate_data(startdate, enddate, resolution, urlPrice, tickers)\n",
					"    \n",
					"    if final_df is not None:\n",
					"        print(f\"Data collected for resolution {resolution}:\")\n",
					"        final_df.show()\n",
					"\n",
					"        # Định dạng ngày hôm nay để đặt tên file\n",
					"        today_str = datetime.now().strftime('%Y%m%d')\n",
					"        file_name = f\"data_{today_str}.json\"\n",
					"        \n",
					"        # Đường dẫn đến ADLS\n",
					"        json_final_path = output_directory + file_name\n",
					"\n",
					"        # Ghi dữ liệu ra file JSON (ghi đè nếu file đã tồn tại)\n",
					"        final_df.coalesce(1).write.mode(\"overwrite\").json(json_final_path)\n",
					"        \n",
					"        print(f\"Data written to ADLS: {json_final_path}\")\n",
					"        \n",
					"        # Return file info for pipeline\n",
					"        return {\n",
					"            \"fileName\": file_name,\n",
					"            \"filePath\": f\"/data/{file_name}\",\n",
					"            \"fullPath\": json_final_path\n",
					"        }\n",
					"    else:\n",
					"        print(\"No data available to write to ADLS!\")\n",
					"        return None\n",
					"\n",
					"# Configure dates\n",
					"timezone = pytz.timezone(\"Asia/Bangkok\")\n",
					"today = datetime.now(timezone)\n",
					"startdate = datetime(today.year, today.month, today.day, 8, 0, 0, tzinfo=timezone)\n",
					"enddate = datetime(today.year, today.month, today.day, 16, 0, 0, tzinfo=timezone)\n",
					"\n",
					"# API URL\n",
					"urlPrice = \"https://banggia.tvs.vn/datafeed/history?\"\n",
					"\n",
					"# Run process and return result for pipeline\n",
					"result = process(startdate, enddate, 1, urlPrice)\n",
					"\n",
					"# Exit notebook with result\n",
					"if result:\n",
					"    notebookOutput = {\n",
					"        \"status\": \"success\",\n",
					"        \"result\": result\n",
					"    }\n",
					"else:\n",
					"    notebookOutput = {\n",
					"        \"status\": \"failed\",\n",
					"        \"result\": None\n",
					"    }\n",
					"\n",
					"mssparkutils.notebook.exit(notebookOutput)\n",
					""
				],
				"execution_count": 9
			}
		]
	}
}