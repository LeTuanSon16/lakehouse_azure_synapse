{
	"name": "Load_data",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f1519cba-bf26-496f-af9a-56980cd33e20"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/a27e5480-6d37-4b13-ae06-aed35c4c2aea/resourceGroups/tmrlakehouse/providers/Microsoft.Synapse/workspaces/tmrlakehouse/bigDataPools/sparkpool",
				"name": "sparkpool",
				"type": "Spark",
				"endpoint": "https://tmrlakehouse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, substring\n",
					"from pyspark.sql.types import (\n",
					"    StructType, StructField, StringType, FloatType, LongType\n",
					")\n",
					"\n",
					"# Initialize Spark Session\n",
					"spark = SparkSession.builder \\\n",
					"    .appName(\"ADLS-to-Synapse\") \\\n",
					"    .config(\"spark.jars.packages\", \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\") \\\n",
					"    .getOrCreate()\n",
					"\n",
					"# Configuration Variables\n",
					"STORAGE_ACCOUNT_NAME = \"tmrbronzelayer\"\n",
					"CONTAINER_NAME = \"securities-price\"\n",
					"ADLS_PATH = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/data/\"\n",
					"TEMP_FOLDER = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/temp/\"\n",
					"\n",
					"SQL_CONFIG = {\n",
					"    \"server\": \"tmrlakehouse.sql.azuresynapse.net\",\n",
					"    \"database\": \"tmr_dwh\",\n",
					"    \"user\": \"sqladminuser\",\n",
					"    \"password\": \"Le tuan $on 03\",\n",
					"    \"table\": \"[dbo].[realtimesdata]\",\n",
					"    \"jdbc_url\": \"jdbc:sqlserver://tmrlakehouse.sql.azuresynapse.net:1433;database=tmr_dwh;\"\n",
					"                 \"encrypt=true;trustServerCertificate=false;loginTimeout=30;\"\n",
					"}\n",
					"\n",
					"# Define Schema - ƒê·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi d·ªØ li·ªáu ngu·ªìn\n",
					"DATA_SCHEMA = StructType([\n",
					"    StructField(\"data_time\", StringType(), True),\n",
					"    StructField(\"data_date\", StringType(), True),\n",
					"    StructField(\"resolution\", LongType(), True),\n",
					"    StructField(\"ticker\", StringType(), True),\n",
					"    StructField(\"open_price\", FloatType(), True),\n",
					"    StructField(\"highest_price\", FloatType(), True),\n",
					"    StructField(\"lowest_price\", FloatType(), True),\n",
					"    StructField(\"close_price\", FloatType(), True),\n",
					"    StructField(\"volume\", FloatType(), True),\n",
					"    StructField(\"created_at\", StringType(), True),\n",
					"    StructField(\"updated_at\", StringType(), True),\n",
					"])\n",
					"\n",
					"def read_json_from_adls(json_path):\n",
					"    \"\"\"Reads JSON data from ADLS and returns a Spark DataFrame.\"\"\"\n",
					"    print(f\"üì• Reading data from ADLS: {json_path}\")\n",
					"    try:\n",
					"        df = spark.read.schema(DATA_SCHEMA).json(json_path)\n",
					"        df.show(5)\n",
					"        return df\n",
					"    except Exception as e:\n",
					"        print(f\"‚ö†Ô∏è Error reading JSON from ADLS: {e}\")\n",
					"        return None\n",
					"\n",
					"def create_synapse_table():\n",
					"    \"\"\"Creates the target table in Synapse if it doesn't exist.\"\"\"\n",
					"    print(f\"üîß Creating table if not exists: {SQL_CONFIG['table']}\")\n",
					"    \n",
					"    # K·∫øt n·ªëi JDBC ƒë·ªÉ th·ª±c thi SQL tr·ª±c ti·∫øp\n",
					"    conn_properties = {\n",
					"        \"user\": SQL_CONFIG['user'],\n",
					"        \"password\": SQL_CONFIG['password'],\n",
					"        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
					"    }\n",
					"    \n",
					"    # SQL t·∫°o b·∫£ng v·ªõi HEAP (kh√¥ng d√πng columnstore) ƒë·ªÉ tr√°nh l·ªói v·ªõi c·ªôt ticker\n",
					"    create_table_sql = f\"\"\"\n",
					"    IF NOT EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'{SQL_CONFIG['table']}') AND type in (N'U'))\n",
					"    BEGIN\n",
					"        CREATE TABLE {SQL_CONFIG['table']} (\n",
					"            data_time VARCHAR(8000),\n",
					"            data_date VARCHAR(8000),\n",
					"            resolution BIGINT,\n",
					"            ticker VARCHAR(8000),\n",
					"            open_price FLOAT, \n",
					"            highest_price FLOAT,\n",
					"            lowest_price FLOAT,\n",
					"            close_price FLOAT,\n",
					"            volume FLOAT,\n",
					"            created_at VARCHAR(8000),\n",
					"            updated_at VARCHAR(8000)\n",
					"        ) WITH (HEAP);\n",
					"    END\n",
					"    \"\"\"\n",
					"    \n",
					"    try:\n",
					"        # Th·ª±c thi c√¢u l·ªánh SQL th√¥ng qua JDBC\n",
					"        from py4j.java_gateway import java_import\n",
					"        java_import(spark._sc._jvm, \"java.sql.DriverManager\")\n",
					"        java_import(spark._sc._jvm, \"java.sql.Connection\")\n",
					"        java_import(spark._sc._jvm, \"java.sql.Statement\")\n",
					"        \n",
					"        conn = spark._sc._jvm.DriverManager.getConnection(\n",
					"            SQL_CONFIG['jdbc_url'],\n",
					"            SQL_CONFIG['user'],\n",
					"            SQL_CONFIG['password']\n",
					"        )\n",
					"        stmt = conn.createStatement()\n",
					"        stmt.execute(create_table_sql)\n",
					"        stmt.close()\n",
					"        conn.close()\n",
					"        \n",
					"        print(f\"‚úÖ Table {SQL_CONFIG['table']} created successfully or already exists\")\n",
					"    except Exception as e:\n",
					"        print(f\"‚ùå Error creating table: {e}\")\n",
					"\n",
					"def write_to_synapse(df):\n",
					"    \"\"\"Writes DataFrame to Azure Synapse Dedicated SQL Pool using JDBC.\"\"\"\n",
					"    print(f\"üì§ Writing data to Synapse Table: {SQL_CONFIG['table']}\")\n",
					"    \n",
					"    try:\n",
					"        # Ghi DataFrame v√†o Synapse\n",
					"        df.write \\\n",
					"            .format(\"jdbc\") \\\n",
					"            .option(\"url\", SQL_CONFIG['jdbc_url']) \\\n",
					"            .option(\"dbtable\", SQL_CONFIG['table']) \\\n",
					"            .option(\"user\", SQL_CONFIG['user']) \\\n",
					"            .option(\"password\", SQL_CONFIG['password']) \\\n",
					"            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
					"            .option(\"batchsize\", 5000) \\\n",
					"            .mode(\"append\") \\\n",
					"            .save()\n",
					"        \n",
					"        print(f\"‚úÖ Data successfully written to {SQL_CONFIG['table']}\")\n",
					"    except Exception as e:\n",
					"        print(f\"‚ùå Error writing to Synapse Table: {e}\")\n",
					"        print(f\"Error details: {str(e)}\")\n",
					"\n",
					"def main():\n",
					"    # T·∫°o b·∫£ng tr∆∞·ªõc khi ƒë·ªçc d·ªØ li·ªáu\n",
					"    create_synapse_table()\n",
					"    \n",
					"    df = read_json_from_adls(ADLS_PATH + \"data_*.json\")\n",
					"    if df and not df.isEmpty():\n",
					"        write_to_synapse(df)\n",
					"    else:\n",
					"        print(\"‚ö†Ô∏è No valid data found for processing.\")\n",
					"\n",
					"if __name__ == \"__main__\":\n",
					"    main()"
				],
				"execution_count": 7
			}
		]
	}
}